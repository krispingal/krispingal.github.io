<!DOCTYPE html>
<html lang="en-us" dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=2.5">
  <meta name="description" content="How do you predict the life paths of cats and dogs in shelters? This journey takes you through Kaggle’s shelter animal outcomes competition, from feature engineering to parameter tuning, with lessons learned at every step.">
<title>Shelter Animal Outcomes | kbabuji</title>
    <link rel="stylesheet" href="/css/main.css">
      <script src="/js/main.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: '\\[', right: '\\]', display: true },   
                { left: '$$', right: '$$', display: true },     
                { left: '\\(', right: '\\)', display: false },  
            ],
            throwOnError: false
        });
    });
</script><script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "\"Shelter Animal Outcomes\"",
  "description": "\"How do you predict the life paths of cats and dogs in shelters? This journey takes you through Kaggle’s shelter animal outcomes competition, from feature engineering to parameter tuning, with lessons learned at every step.\"",
  "articleBody": "\" A puppy taken in at a shelter Picture taken from SOS\\nShelter animal outcomes is a knowledge competition hosted by kaggle. My goal was to learn and get familiarized with the different techniques, methods and tools required to solve classification problems. Throughout this journey, I utilized pandas, matplotlib, seaborn, jupyter and scikit-learn. The goal of this competition is to predict the outcome of a cat or a dog, based on its age, gender(also whether spayed/neutered or not), breed, color. The outcomes are return-to-owner, adoption, transfer, euthanasia and died. In some cases we are provided with outcome subtypes too.\\nAll my work related to this can be found in this github repository, it houses my jupyter notebooks.\\nPrior beliefs # Before looking at the competition data, I thought cats might be a bit more favored than dogs, because of seeing many cat memes \\u0026amp; gifs. I thought the following factors to influence the outcome of an animal, in descending order of importance.\\ncat/dog age breed color gender The data # The training set had the following features:\\nAnimalType : Cat/Dog AgeUponOutcome : age Breed SexUponOutcome : gender + spayed/neutered/intact Color DateTime Name OutcomeType [Adoption, Transfer, Euthanasia, Death \\u0026amp; Return to owner] \\u0026amp; OutcomeSubtype [Foster, Partner, Surgery, Suffering \\u0026hellip;] The test data does not include the last two columns. Perhaps OutcomeSubtype column was just provided to satisfy the participant\\u0026rsquo;s curiosity.\\nModel creation # To reach my objective, to learn about various methods and techniques to solve similar problems, I planned to start off with the simplest models then progressively fine tune it. In all I made three passes and learned new things in every one of them.\\nI used the following algorithms for my models:\\nRandom Forests Logistic Regression Naive Bayes Decision Tree SVM KNN Extra Trees Classifier AdaBoost GradientTreeBoosting and Bagging. First pass # I started off by doing a bit of visualization. The visualization gave me some knowledge on how the data is spread and what features might be important. Next I did a crude feature manipulation and feature selection to finally get the following features: AnimalType, AgeInDays, Sex+Neutered, HasName. This data was fed into Scikit-learn\\u0026rsquo;s algorithms to generate predictions. I tried to use as many algorithms I could but the evaluation metric log loss would need to get probability confidence for each outcome.\\n\\\\[ \\\\text{log loss} = -\\\\frac{1}{N}\\\\sum_{i=1}^{N} \\\\sum_{j=1}^{M} y_{ij}\\\\log(p_{ij}) \\\\tag{1} \\\\]\\nApart from using the score which Kaggle provides, I used cross-validation to get an estimate on how well the models are doing. The results of this initial pass were satisfactory for a first attempt.\\nKey learning point for me from the first pass was, I got familiar with scikit-learn\\u0026rsquo;s api for classification tasks as well as cross-validation.\\nSecond pass # This time when I browsed through the forums for this competition, I found out a post which talked out about a potential data leak. This made me realize that features like AgeUponOutcome, DateTime are actually data leaks and should not be used in the model. Hence I decided to do a better feature transformation or data cleaning this time, which would not include any data leaks. I decided to keep a feature about gender + neutered or not, because the animal can be spayed/neutered if the new owners request for it(in case of adoption or fostering).\\nFinally I ended up with the following features:\\nNameLength, IsMix, CoatType(short/medium/long hair), IsDangerous(pit bull or other dangerous dogs), Gender + spayed/neutered, Ancestor1_group and Ancestor2_group. The Ancestor_group features are for dogs, where dog breeds are classified into groups like sporting, herding, hound, working etc. I got this idea from another post in the forum.\\nApart from the above changes in feature transformation, in the second pass I made additional changes to do feature selection as well. Wherever possible I tried using scikit-learn\\u0026rsquo;s RFECV(Recursive Feature Elimination with Cross Validation). It is not always the best method for feature selection though. With algorithms where RFECV won\\u0026rsquo;t work I used univariate feature elimination. For some classifiers I used pipelines which pipe feature selection with the classification algorithms.\\nI did notice most classifiers were not doing as well as it did during the first pass, probably because I removed the Age feature which should have been a feature that played a crucial role of the outcome of an animal.\\nKey learning points in this pass was learning about Feature selection and pipelines.\\nThird pass # For the third and final pass my aim was to tune the classifiers with their key parameters to achieve the best scores. For this I used scikit-learn\\u0026rsquo;s GridSearchCV. Essentially you give a parameter or a list of parameters and the list of values these can take, then internally it will do a search over all the given parameters, and finally it will give you the combination which gives the best score. As you can imagine this is a computationally intensive operation.\\nFor certain classifiers, like SVM and GradientTreeBoosting classsifiers, GridSearchCV even running on multiple cores could take hours to complete. In these cases I used the alternative RandomizedSearchCV, in which you can specify how many iterations it should perform. One can also provide random numbers from a range, for these parameters. Hence if you have low computational resources and want a solution which is good but not the best,\\nin limited time, this is a good solution, if you set low number of iterations. RandomizedSearchCV can also be used if you have ample computational resources and no time restrictions to iterate through many parameters that you might not want to list down. The drawback for RandomizedSearchCV is you might repeat an iteration with a set of parameters that might have already been tried.\\nKey learning points were learning about GridSearchCV and RandomizedSearchCV. One thing I learned afterwards was that, in practice GridSearchCV is only provided the parameters that are deemed important for that classifier.\\nConclusion # For me this project, when considering everything has been highly rewarding. I got familiar with tools like pandas and scikit-learn. I also got to learn about how to use apply techniques like feature selection and parameter tuning.\\nI knew that I could have obtained better kaggle score if I exploited some data leaks, or used outside data to train a better classifier. I believed that these activities were not with the spirit of the competition nor do they take me closer to my objective, so I did not spend time pursuing these paths.\\nI also got to learn more about cats and dogs, which dispelled some of the presumptions I held in my mind.\\nP.S.: This post and related work are dedicated to my family\\u0026rsquo;s cat, who passed away on August 20.\\n\"",
  "image": ["\"http://localhost:1313/puppy.webp\""],
  "datePublished": "2016-08-24T04:14:54-08:00",
  "dateModified": "2016-08-24T04:14:54-08:00",
  "url": "\"http://localhost:1313/articles/shelter-animal-outcomes/\"",
  "mainEntityOfPage": "\"http://localhost:1313/articles/shelter-animal-outcomes/\"",
  "author": {
    "@type": "Person",
    "name": "\"Krishna Babuji\"",
    "url": "\"http://localhost:1313/about\"",
    "image": "\"http://localhost:1313/about/images/portrait.webp\""
  },
  "publisher": {
    "@type": "Organization",
    "name": "\"kbabuji\"",
    "logo": {
      "@type": "ImageObject",
      "url": "\"http://localhost:1313/favicon-32x32.png\""
    }
  },
  "articleSection": "\"aiml\"",
  "keywords": "null",
  "wordCount":  1107 
}
</script>
<meta property="og:url" content="http://localhost:1313/articles/shelter-animal-outcomes/">
  <meta property="og:site_name" content="kbabuji">
  <meta property="og:title" content="Shelter Animal Outcomes">
  <meta property="og:description" content="How do you predict the life paths of cats and dogs in shelters? This journey takes you through Kaggle’s shelter animal outcomes competition, from feature engineering to parameter tuning, with lessons learned at every step.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2016-08-24T04:14:54-08:00">
    <meta property="article:modified_time" content="2016-08-24T04:14:54-08:00">
    <meta property="og:image" content="http://localhost:1313/articles/shelter-animal-outcomes/puppy.webp">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
</head>

<body>
  <header class="navbar">
    <div class="masthead">
    <h2><a class="site-title" href="/">kbabuji</a></h2>
    <nav class="nav-links">
        <a href="/about" aria-label="About me">About</a>
        <a href="/articles">Archives</a>
        <a href="/categories">Categories</a>
        <a href="/tags">Tags</a>
    </nav>
</div>
  </header>
  <main class="wrapper">
    
<article class="article-container">
    
    <header class="article-header">
        <div class="category">AI &amp; ML</div>
        <h1 class="title">Shelter Animal Outcomes</h1>
        <p class="description">How do you predict the life paths of cats and dogs in shelters? This journey takes you through Kaggle’s shelter animal outcomes competition, from feature engineering to parameter tuning, with lessons learned at every step.</p>
        <div class="meta">
            <time datetime="2016-08-24">
                August 24, 2016
            </time>
        </div>
    </header>
    <div class="minimal-divider"></div>

    
    <div class="content">
        <figure><img src="/articles/shelter-animal-outcomes/puppy.webp"
    alt="Image of a puppt taken in at a shelter" loading="lazy"><figcaption>
      <h4>A puppy taken in at a shelter</h4><p>
          <a href="https://www.flickr.com/photos/pifaslt/">Picture taken from SOS</a></p>
    </figcaption>
</figure>

<p><a href="https://www.kaggle.com/c/shelter-animal-outcomes">Shelter animal outcomes</a> is a knowledge
competition hosted by <a href="https://www.kaggle.com/">kaggle</a>. My goal was to learn and get
familiarized with the different techniques, methods and tools required to
solve classification problems. Throughout this journey, I utilized <a href="http://pandas.pydata.org/">pandas</a>,
<a href="http://matplotlib.org/">matplotlib</a>, <a href="https://stanford.edu/~mwaskom/software/seaborn/index.html">seaborn</a>, <a href="http://jupyter.org/">jupyter</a> and
<a href="http://scikit-learn.org/stable/index.html">scikit-learn</a>. The goal of this competition
is to predict the outcome of a cat or a dog, based on its age, gender(also
whether spayed/neutered or not), breed, color. The outcomes are
return-to-owner, adoption, transfer, euthanasia and died. In some cases
we are provided with outcome subtypes too.</p>
<p>All my work related to this can be found in this
<a href="https://github.com/krispingal/shelterAnimalOutcomes">github repository</a>, it houses my jupyter notebooks.</p>
<h1 id="prior-beliefs">
  Prior beliefs
  <a class="anchor" href="#prior-beliefs" aria-label="Link to section - Prior beliefs">#</a>
</h1>

<p>Before looking at the competition data, I thought cats might be a bit
more favored than dogs, because of seeing many cat memes &amp; gifs. I
thought the following factors to influence the outcome of an animal,
in descending order of importance.</p>
<ol>
<li>cat/dog</li>
<li>age</li>
<li>breed</li>
<li>color</li>
<li>gender</li>
</ol>
<h1 id="the-data">
  The data
  <a class="anchor" href="#the-data" aria-label="Link to section - The data">#</a>
</h1>

<p>The training set had the following features:</p>
<ul>
<li>AnimalType : Cat/Dog</li>
<li>AgeUponOutcome : age</li>
<li>Breed</li>
<li>SexUponOutcome : gender + spayed/neutered/intact</li>
<li>Color</li>
<li>DateTime</li>
<li>Name</li>
<li><em>OutcomeType</em> [Adoption, Transfer, Euthanasia, Death &amp; Return to owner] &amp;</li>
<li>OutcomeSubtype [Foster, Partner, Surgery, Suffering &hellip;]</li>
</ul>
<p>The test data does not include the last two columns.
Perhaps OutcomeSubtype column was just provided to satisfy the participant&rsquo;s curiosity.</p>
<h1 id="model-creation">
  Model creation
  <a class="anchor" href="#model-creation" aria-label="Link to section - Model creation">#</a>
</h1>

<p>To reach my objective, to learn about various methods and techniques to solve similar problems,
I planned to start off with the simplest models then progressively fine tune it. In all I made three
passes and learned new things in every one of them.</p>
<p>I used the following algorithms for my models:</p>
<ol>
<li>Random Forests</li>
<li>Logistic Regression</li>
<li>Naive Bayes</li>
<li>Decision Tree</li>
<li>SVM</li>
<li>KNN</li>
<li>Extra Trees Classifier</li>
<li>AdaBoost</li>
<li>GradientTreeBoosting</li>
<li>and Bagging.</li>
</ol>
<hr>
<h1 id="first-pass">
  First pass
  <a class="anchor" href="#first-pass" aria-label="Link to section - First pass">#</a>
</h1>

<p>I started off by doing a bit of visualization. The visualization gave me
some knowledge on how the data is spread and what features might be
important. Next I did a crude feature manipulation and feature selection
to finally get the following features: <code>AnimalType</code>, <code>AgeInDays</code>,
<code>Sex+Neutered</code>, <code>HasName</code>. This data was fed into Scikit-learn&rsquo;s algorithms
to generate predictions. I tried to use as many algorithms I
could but the evaluation metric log loss would need to get probability
confidence for each outcome.</p>
<p>\[
\text{log loss} = -\frac{1}{N}\sum_{i=1}^{N} \sum_{j=1}^{M} y_{ij}\log(p_{ij}) \tag{1}
\]</p>
<p>Apart from using the score which Kaggle provides, I used cross-validation
to get an estimate on how well the models are doing.
The results of this initial pass were satisfactory for a first attempt.</p>
<p>Key learning point for me from the first pass was, I got familiar
with scikit-learn&rsquo;s api for classification tasks as well as
cross-validation.</p>
<hr>
<h1 id="second-pass">
  Second pass
  <a class="anchor" href="#second-pass" aria-label="Link to section - Second pass">#</a>
</h1>

<p>This time when I browsed through the forums for this competition, I
found out a <a href="https://www.kaggle.com/c/shelter-animal-outcomes/forums/t/22119/cheating-your-way-to-the-top-of-the-lb-remove-the-lb">post</a> which talked out about a potential data
leak. This made me realize that features like AgeUponOutcome, DateTime
are actually data leaks and should not be used in the model. Hence I
decided to do a better feature transformation or data cleaning this
time, which would not include any data leaks. I decided to keep a
feature about gender + neutered or not, because the animal can be
spayed/neutered if the new owners request for it(in case of adoption or
fostering).</p>
<p>Finally I ended up with the following features:</p>
<ul>
<li><code>NameLength</code>,</li>
<li><code>IsMix</code>,</li>
<li><code>CoatType</code>(short/medium/long hair),</li>
<li><code>IsDangerous</code>(pit bull or other dangerous dogs),</li>
<li>Gender + spayed/neutered,</li>
<li>Ancestor1_group and Ancestor2_group.</li>
</ul>
<p>The Ancestor_group features are for dogs, where dog breeds are
classified into groups like sporting, herding, hound, working etc.
I got this idea from another <a href="https://www.kaggle.com/andraszsom/shelter-animal-outcomes/dog-breeds-dog-groups/discussion">post</a> in the forum.</p>
<p>Apart from the above changes in feature transformation, in the second
pass I made additional changes to do feature selection as well. Wherever
possible I tried using scikit-learn&rsquo;s RFECV(Recursive Feature Elimination
with Cross Validation). It is not always the best method for feature
selection though. With algorithms where RFECV won&rsquo;t work I used
univariate feature elimination. For some classifiers I used pipelines
which pipe feature selection with the classification algorithms.</p>
<p>I did notice most <em>classifiers were not doing as well as it did during the
first pass</em>, probably because I removed the Age feature which should
have been a feature that played a crucial role of the outcome of an
animal.</p>
<p>Key learning points in this pass was learning about Feature selection and
pipelines.</p>
<hr>
<h1 id="third-pass">
  Third pass
  <a class="anchor" href="#third-pass" aria-label="Link to section - Third pass">#</a>
</h1>

<p>For the third and final pass my aim was to tune the classifiers with
their key parameters to achieve the best scores. For this I used
scikit-learn&rsquo;s <em>GridSearchCV</em>. Essentially you give a parameter or a list
of parameters and the list of values these can take, then internally it
will do a search over all the given parameters, and finally it will give
you the combination which gives the best score. As you can imagine this
is a computationally intensive operation.</p>
<p>For certain classifiers, like SVM and GradientTreeBoosting classsifiers,
GridSearchCV even running on multiple cores could take hours to complete.
In these cases I used the alternative <em>RandomizedSearchCV</em>, in which you
can specify how many iterations it should perform. One can also provide
random numbers from a range, for these parameters. Hence if you have low
computational resources and want a solution which is good but not the best,<br>
in limited time, this is a good solution, if you set low number of
iterations. RandomizedSearchCV can also be used if you have ample
computational resources and no time restrictions to iterate
through many parameters that you might not want to list down. The
drawback for RandomizedSearchCV is you might repeat an iteration with a
set of parameters that might have already been tried.</p>
<p>Key learning points were learning about GridSearchCV and
RandomizedSearchCV. One thing I learned afterwards was that, <em>in practice
GridSearchCV is only provided the parameters that are deemed important
for that classifier</em>.</p>
<hr>
<h1 id="conclusion">
  Conclusion
  <a class="anchor" href="#conclusion" aria-label="Link to section - Conclusion">#</a>
</h1>

<p>For me this project, when considering everything has been highly rewarding.
I got familiar with tools like pandas and scikit-learn. I also got to
learn about how to use apply techniques like feature selection and
parameter tuning.</p>
<p>I knew that I could have obtained better kaggle score if I exploited
some data leaks, or used outside data to train a better classifier. I
believed that these activities were not with the spirit of the
competition nor do they take me closer to my objective, so I did not
spend time pursuing these paths.</p>
<p>I also got to learn more about cats and dogs, which dispelled some of the
presumptions I held in my mind.</p>

  
  
    <blockquote>
      <p>P.S.: This post and related work are dedicated to my family&rsquo;s cat, who passed away on August 20.</p>
    </blockquote>
    </div>

    
    <footer class="article-footer">
        <nav class="article-tags">

        </nav>
    </footer>
</article>
<aside class="related-posts">
</aside>
<div class="content">
    <script src="https://giscus.app/client.js"
    data-repo="krispingal/krispingal.github.io"
    data-repo-id="R_kgDONNB2Ng"
    data-category="Announcements"
    data-category-id="DIC_kwDONNB2Ns4CmciE"
    data-mapping="url"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="catppuccin_latte"
    data-lang="en"
    data-loading="lazy"
    crossorigin="anonymous"
    async>
  </script>
</div>
  </main>
  <footer class="footer-nav">
<div class="footer-icons">
    <a href="/" aria-label="Home"><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m21.743 12.331-9-10c-.379-.422-1.107-.422-1.486 0l-9 10a.998.998 0 0 0-.17 1.076c.16.361.518.593.913.593h2v7a1 1 0 0 0 1 1h3a1 1 0 0 0 1-1v-4h4v4a1 1 0 0 0 1 1h3a1 1 0 0 0 1-1v-7h2a.998.998 0 0 0 .743-1.669z"></path></svg></a>
    <a href="https://github.com/krispingal" target="_blank" aria-label="GitHub"><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path></svg></a>
    <a href="https://linkedin.com/in/krishna-babuji" target="_blank" aria-label="LinkedIn"><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 3H4a1 1 0 0 0-1 1v16a1 1 0 0 0 1 1h16a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM8.339 18.337H5.667v-8.59h2.672v8.59zM7.003 8.574a1.548 1.548 0 1 1 0-3.096 1.548 1.548 0 0 1 0 3.096zm11.335 9.763h-2.669V14.16c0-.996-.018-2.277-1.388-2.277-1.39 0-1.601 1.086-1.601 2.207v4.248h-2.667v-8.59h2.56v1.174h.037c.355-.675 1.227-1.387 2.524-1.387 2.704 0 3.203 1.778 3.203 4.092v4.71z"></path></svg></a>
    <a href="mailto:krishna.pingal@gmail.com" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon"><path d="M20 4H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2zm0 4.7-8 5.334L4 8.7V6.297l8 5.333 8-5.333V8.7z"></path></svg></a>
    <a href="/articles/index.xml" aria-label="RSS"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon"><path d="M19 20.001C19 11.729 12.271 5 4 5v2c7.168 0 13 5.832 13 13.001h2z"></path><path d="M12 20.001h2C14 14.486 9.514 10 4 10v2c4.411 0 8 3.589 8 8.001z"></path><circle cx="6" cy="18" r="2"></circle></svg></a>
</div>
<p class="footer-copyright">© 2016 &ndash; 2025 kbabuji. All rights reserved.</p>

  </footer>
</body>

</html>