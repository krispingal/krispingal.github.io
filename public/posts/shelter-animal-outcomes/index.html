<!DOCTYPE html>
<html lang="en-us" dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Shelter Animal Outcomes | kbabuji</title>

    <link rel="stylesheet" href="/css/main.css">


      <script src="/js/main.js"></script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: '\\[', right: '\\]', display: true },   
                { left: '$$', right: '$$', display: true },     
                { left: '\\(', right: '\\)', display: false },  
            ],
            throwOnError: false
        });
    });
</script>


</head>

<body>
  <header>
    <h2>kbabuji</h2>


  <nav>
    <ul>
    <li>
      <a href="/about/">About</a>
    </li>
    <li>
      <a href="/archive/">Archive</a>
    </li>
    <li>
      <a href="/">Home</a>
    </li>
    <li>
      <a href="/tags/">Tags</a>
    </li>
    </ul>
  </nav>


  </header>
  <main>
    
<h1>Shelter Animal Outcomes</h1>



<time datetime="2016-08-24T04:14:54-08:00">Aug 24, 2016</time>
  <p>{% include images.html img=&quot;/assets/img/puppy.webp&quot; title=&ldquo;A puppy taken in at a shelter&rdquo; caption=&ldquo;A puppy taken in at a shelter.&rdquo; %}</p>
<!-- raw HTML omitted -->
<p><a href="https://www.kaggle.com/c/shelter-animal-outcomes">Shelter animal outcomes</a> is a knowledge
competition hosted by <a href="https://www.kaggle.com/">kaggle</a>. My goal was to learn and get
familiarized with the different techniques, methods and tools required to
solve classification problems. In my adventures I used <a href="http://pandas.pydata.org/">pandas</a>,
<a href="http://matplotlib.org/">matplotlib</a>, <a href="https://stanford.edu/~mwaskom/software/seaborn/index.html">seaborn</a>, <a href="http://jupyter.org/">jupyter</a> and
<a href="http://scikit-learn.org/stable/index.html">scikit-learn</a>. The goal of this competition
is to predict the outcome of a cat or a dog, given it&rsquo;s age, gender(also
whether spayed/neutered or not), breed, color. The outcomes are
return-to-owner, adoption, transfer, euthanasia and died. In some cases
we are provided with outcome subtypes too.</p>
<p>All my work related to this can be found in this
<a href="https://github.com/krispingal/shelterAnimalOutcomes">github repository</a>, it houses my jupyter notebooks.</p>
<h1 id="prior-beliefs">
  Prior beliefs
  <a href="#prior-beliefs">#</a>
</h1>

<p>Before looking at the competition data, I thought cats might be a bit
more favored than dogs, because of seeing many cat memes &amp; gifs. I
thought the following factors to influence the outcome of an animal,
in descending order of importance.</p>
<ol>
<li>cat/dog</li>
<li>age</li>
<li>breed</li>
<li>color</li>
<li>gender</li>
</ol>
<h1 id="the-data">
  The data
  <a href="#the-data">#</a>
</h1>

<p>The training set had the following features:</p>
<ul>
<li>AnimalType : Cat/Dog</li>
<li>AgeUponOutcome : age</li>
<li>Breed</li>
<li>SexUponOutcome : gender + spayed/neutered/intact</li>
<li>Color</li>
<li>DateTime</li>
<li>Name</li>
<li><em>OutcomeType</em> [Adoption, Transfer, Euthanasia, Death &amp; Return to owner] &amp;</li>
<li>OutcomeSubtype [Foster, Partner, Surgery, Suffering &hellip;]</li>
</ul>
<p>The test data does not include the last two columns.
Perhaps OutcomeSubtype column was just provided to satisfy the participant&rsquo;s curiosity.</p>
<h1 id="model-creation">
  Model creation
  <a href="#model-creation">#</a>
</h1>

<p>To reach my objective, to learn about various methods and techniques to solve similar problems,
I planned to start off with the simplest models then progressively fine tune it. In all I made three
passes and learned new things in every one of them.</p>
<p>I used the following algorithms for my models:</p>
<ol>
<li>Random Forests</li>
<li>Logistic Regression</li>
<li>Naive Bayes</li>
<li>Decision Tree</li>
<li>SVM</li>
<li>KNN</li>
<li>Extra Trees Classifier</li>
<li>AdaBoost</li>
<li>GradientTreeBoosting</li>
<li>and Bagging.</li>
</ol>
<hr>
<h1 id="first-pass">
  First pass
  <a href="#first-pass">#</a>
</h1>

<p>I started off by doing a bit of visualization. The visualization gave me
some knowledge on how the data is spread and what features might be
important. Next I did a crude feature manipulation and feature selection
to finally get the following features: <code>AnimalType</code>, <code>AgeInDays</code>,
<code>Sex+Neutered</code>, <code>HasName</code>. Finally I feed this data to scikit-learn&rsquo;s
algorithms to get predictions. I tried to use as many algorithms I
could but the evaluation metric log loss would need to get probability
confidence for each outcome.</p>
<p>\[
\text{log loss} = -\frac{1}{N}\sum_{i=1}^{N} \sum_{j=1}^{M} y_{ij}\log(p_{ij}) \tag{1}
\]</p>
<p>Apart from using the score which Kaggle provides, I used cross-validation
to get an estimate on how well the models are doing. I got scores which
I was pretty much satisfied for an in initial pass.</p>
<p>Key learning point for me from the first pass was, I got familiar
with scikit-learn&rsquo;s api for classification tasks as well as
cross-validation.</p>
<hr>
<h1 id="second-pass">
  Second pass
  <a href="#second-pass">#</a>
</h1>

<p>This time when I browsed through the forums for this competition, I
found out a <a href="https://www.kaggle.com/c/shelter-animal-outcomes/forums/t/22119/cheating-your-way-to-the-top-of-the-lb-remove-the-lb">post</a> which talked out about a potential data
leak. This made me realize that features like AgeUponOutcome, DateTime
are actually data leaks and should not be used in the model. Hence I
decided to do a better feature transformation or data cleaning this
time, which would not include any data leaks. I decided to keep a
feature about gender + neutered or not, because the animal can be
spayed/neutered if the new owners request for it(in case of adoption or
fostering).</p>
<p>Finally I ended up with the following features:</p>
<ul>
<li><code>NameLength</code>,</li>
<li><code>IsMix</code>,</li>
<li><code>CoatType</code>(short/medium/long hair),</li>
<li><code>IsDangerous</code>(pit bull or other dangerous dogs),</li>
<li>Gender + spayed/neutered,</li>
<li>Ancestor1_group and Ancestor2_group.</li>
</ul>
<p>The Ancestor_group features are for dogs, where dog breeds are
classified into groups like sporting, herding, hound, working etc.
I got this idea from another <a href="https://www.kaggle.com/andraszsom/shelter-animal-outcomes/dog-breeds-dog-groups/discussion">post</a> in the forum.</p>
<p>Apart from the above changes in feature transformation, in the second
pass I made additional changes to do feature selection as well. Wherever
possible I tried using scikit-learn&rsquo;s RFECV(Recursive Feature Elimination
with Cross Validation). It is not always the best method for feature
selection though. With algorithms where RFECV won&rsquo;t work I used
univariate feature elimination. For some classifiers I used pipelines
which pipe feature selection with the classification algorithms.</p>
<p>I did notice most <em>classifiers were not doing as well as it did during the
first pass</em>, probably because I removed the Age feature which should
have been a feature that played a crucial role of the outcome of an
animal.</p>
<p>Key learning points in this pass was learning about Feature selection and
pipelines.</p>
<hr>
<h1 id="third-pass">
  Third pass
  <a href="#third-pass">#</a>
</h1>

<p>For the third and final pass my aim was to tune the classifiers with
their key parameters to achieve the best scores. For this I used
scikit-learn&rsquo;s <em>GridSearchCV</em>. Essentially you give a parameter or a list
of parameters and the list of values these can take, then internally it
will do a search over all the given parameters, and finally it will give
you the combination which gives the best score. As you can imagine this
is a computationally intensive operation.</p>
<p>For certain classifiers, like SVM and gradientBoosting classsifiers,
GridSearchCV even running on multiple cores could take hours to complete.
In these cases I used the alternative <em>RandomizedSearchCV</em>, in which you
can specify how many iterations it should perform. One can also provide
random numbers from a range, for these parameters. Hence if you have low
computational resources and want a solution which is good but not the best,<br>
in limited time, this is a good solution, if you set low number of
iterations. RandomizedSearchCV can also be used if you have plenty of
computational resource and no set restrictions on time to iterate
through many parameters that you might not want to list down. The
drawback for RandomizedSearchCV is you might repeat an iteration with a
set of parameters that might have already been tried.</p>
<p>Key learning points were learning about GridSearchCV and
RandomizedSearchCV. One thing I learned afterwards was that, <em>in practice
GridSearchCV is only provided the parameters that are deemed important
for that classifier</em>.</p>
<hr>
<h1 id="conclusion">
  Conclusion
  <a href="#conclusion">#</a>
</h1>

<p>For me this project, when considering everything has been highly rewarding.
I got familiar with tools like pandas and scikit-learn. I also got to
learn about how to use apply techniques like feature selection and
parameter tuning.</p>
<p>I knew that I could have obtained better kaggle score if I exploited
some data leaks, or used outside data to train a better classifier. I
believed that these activities were not with the spirit of the
competition nor do they take me closer to my objective, so I did not
spend time pursuing these paths.</p>
<p>I also got to learn more about cats and dogs, which dispelled some of the
presumptions I held in my mind.</p>
<blockquote>
<p>P.S: This post and related work is dedicated to my family&rsquo;s cat that
passed away on 20 Aug. You are dearly missed. RIP kitty.</p>
</blockquote>
<p>Picture taken from <a href="https://www.flickr.com/photos/pifaslt/">SOS animals</a></p>



  </main>
  <footer>
    <p>Copyright 2024. All rights reserved.</p>

  </footer>
</body>

</html>