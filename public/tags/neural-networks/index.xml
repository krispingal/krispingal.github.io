<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Networks on kbabuji</title>
    <link>http://localhost:1313/tags/neural-networks/</link>
    <description>Recent content in Neural Networks on kbabuji</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 04 Nov 2018 00:00:00 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A guide to Neural Networks</title>
      <link>http://localhost:1313/articles/guide-to-neural-network/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 -0500</pubDate>
      <guid>http://localhost:1313/articles/guide-to-neural-network/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/articles/guide-to-neural-network/nn.webp&#34;&#xA;    alt=&#34;Fig 1. Neural net with Input layer ∈ ℝ16; hidden layer 1 ∈ ℝ12; hidden layer 2 ∈ ℝ10 and finally a single unit ∈ ℝ in the output layer&#34; loading=&#34;lazy&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Neural Network architecture&lt;/h4&gt;&lt;p&gt;Fig 1. Neural net with Input layer ∈ ℝ&lt;!-- raw HTML omitted --&gt;16&lt;!-- raw HTML omitted --&gt;; hidden layer 1  ∈ ℝ&lt;!-- raw HTML omitted --&gt;12&lt;!-- raw HTML omitted --&gt;; hidden layer 2 ∈ ℝ&lt;!-- raw HTML omitted --&gt;10&lt;!-- raw HTML omitted --&gt; and finally a single unit ∈ ℝ in the output layer&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;  &#xA;  &#xA;    &lt;blockquote&gt;&#xA;      &lt;p&gt;TLDR:&#xA;Neural Networks are computational models that stacks units called neurons into layers to create predictions after we have trained the network. Each unit performs a linear transformation on the data followed by a non-linear transformation to produce a result. Forward propagation computes the network result and backward propagation computes the changes in weights and biases for the network in subsequent epochs. The non-linear activation function supports the network to assume any mathematical function, this is called the Universal Approximation theorem.&lt;/p&gt;&#xA;&#xA;    &lt;/blockquote&gt;&#xA;  &#xA;  &#xA;  &lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-label=&#34;Link to section - Introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Every day we are relying on machine learning models to make our lives simpler and hassle-free. Take your average day, where you start, by reading news and weather predictions, or later in the day when you are checking your emails, we unconsciously are relying on machine learning models that run behind the scenes. The list goes on and on with applications in image recognition used in our gallery, audio recognition in voice assistants. This list keeps growing every day.&lt;/p&gt;&#xA;&lt;p&gt;A few popular classical Machine learning models include Logistic Regression, Linear Regression, Decision Trees, Support Vector Machines, and Naive Bayes models. Although these models are nowadays less commonly used in practice, they are still very useful as understanding them helps to understand the more recent models. Ensemble models such as Random forests and XGBoost are some machine learning models that are commonly used these days.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
